{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>canonical_url</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>connect_from</th>\n",
       "      <th>published_at</th>\n",
       "      <th>first_seen_at</th>\n",
       "      <th>last_updated_at</th>\n",
       "      <th>id</th>\n",
       "      <th>producer_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>urls</th>\n",
       "      <th>keywords</th>\n",
       "      <th>tags</th>\n",
       "      <th>metadata</th>\n",
       "      <th>comments</th>\n",
       "      <th>image_urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1585778667</td>\n",
       "      <td>https://www.ptt.cc/bbs/HatePolitics/M.15829640...</td>\n",
       "      <td>Re: [討論] 民進黨是如何騙倒台灣多數人的？</td>\n",
       "      <td>ilw4e (可以吃嗎？)</td>\n",
       "      <td>223.136.226.238</td>\n",
       "      <td>2020-03-01 00:14:44</td>\n",
       "      <td>2020-02-29 16:17:21</td>\n",
       "      <td>2020-04-02 06:04:27</td>\n",
       "      <td>249c7ead-8254-11ea-8627-f23c92e71bad</td>\n",
       "      <td>5030a557-81fe-11ea-8627-f23c92e71bad</td>\n",
       "      <td>※ 引述《peoplerock (綽號暱稱)》之銘言：\\n:     最近開始看八卦板和政黑...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'meta-tags': {'og:site_name': 'Ptt 批踢踢實業坊', '...</td>\n",
       "      <td>[{'id': 0, 'reaction': '推', 'author': 'zeuswel...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1585781182</td>\n",
       "      <td>https://www.ptt.cc/bbs/HatePolitics/M.15829670...</td>\n",
       "      <td>Re: [討論] 非法移工要怎麼處理</td>\n",
       "      <td>vancepeng (NIA新手玩家)</td>\n",
       "      <td>111.71.213.166</td>\n",
       "      <td>2020-03-01 01:03:41</td>\n",
       "      <td>2020-02-29 17:19:58</td>\n",
       "      <td>2020-04-02 06:46:22</td>\n",
       "      <td>25e6b6e3-8254-11ea-8627-f23c92e71bad</td>\n",
       "      <td>5030a557-81fe-11ea-8627-f23c92e71bad</td>\n",
       "      <td>非法移工早就該處理了\\n現在可能成為防疫破口我也不意外\\n\\n為什麼會有非法勞工？\\n立法院...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'meta-tags': {'og:site_name': 'Ptt 批踢踢實業坊', '...</td>\n",
       "      <td>[{'id': 0, 'reaction': '推', 'author': 'yesfay'...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1585781179</td>\n",
       "      <td>https://www.ptt.cc/bbs/HatePolitics/M.15829671...</td>\n",
       "      <td>[討論] 我是柯粉,陳時中做的很好,但是可以提昇</td>\n",
       "      <td>CCTing (逃難回來囉,水有毒)</td>\n",
       "      <td>150.116.216.247</td>\n",
       "      <td>2020-03-01 01:05:14</td>\n",
       "      <td>2020-02-29 17:19:55</td>\n",
       "      <td>2020-04-02 06:46:19</td>\n",
       "      <td>25e639e6-8254-11ea-8627-f23c92e71bad</td>\n",
       "      <td>5030a557-81fe-11ea-8627-f23c92e71bad</td>\n",
       "      <td>先說結論\\n\\n陳時中我給他90分的高分\\n\\n但是柯文哲給的建議還是要聽\\n\\n公開透明才...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'meta-tags': {'og:site_name': 'Ptt 批踢踢實業坊', '...</td>\n",
       "      <td>[{'id': 0, 'reaction': '推', 'author': 'Moratti...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1585781179</td>\n",
       "      <td>https://www.ptt.cc/bbs/HatePolitics/M.15829671...</td>\n",
       "      <td>[討論] 其實台灣防疫工作作的不錯了！</td>\n",
       "      <td>fordmvp (無名氏)</td>\n",
       "      <td>59.126.170.22</td>\n",
       "      <td>2020-03-01 01:05:42</td>\n",
       "      <td>2020-02-29 17:19:54</td>\n",
       "      <td>2020-04-02 06:46:19</td>\n",
       "      <td>25e6255f-8254-11ea-8627-f23c92e71bad</td>\n",
       "      <td>5030a557-81fe-11ea-8627-f23c92e71bad</td>\n",
       "      <td>台灣對比日韓，很多地方都做的不錯了！\\n\\n口罩的管制、預防囤積、哄抬價格、禁止出口等\\n\\...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'meta-tags': {'og:site_name': 'Ptt 批踢踢實業坊', '...</td>\n",
       "      <td>[{'id': 0, 'reaction': '→', 'author': 'XXXXLIN...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1585781172</td>\n",
       "      <td>https://www.ptt.cc/bbs/HatePolitics/M.15829676...</td>\n",
       "      <td>[新聞] 新新聞》抗疫升級重要推手，侯友宜讓蘇貞</td>\n",
       "      <td>knnji (咖啡因過量)</td>\n",
       "      <td>110.50.151.162</td>\n",
       "      <td>2020-03-01 01:14:03</td>\n",
       "      <td>2020-02-29 17:19:52</td>\n",
       "      <td>2020-04-02 06:46:12</td>\n",
       "      <td>25e58aea-8254-11ea-8627-f23c92e71bad</td>\n",
       "      <td>5030a557-81fe-11ea-8627-f23c92e71bad</td>\n",
       "      <td>新新聞》抗疫升級重要推手，侯友宜讓蘇貞昌「聽話」20分鐘\\n\\n新新聞李順德  2020-0...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://www.storm.mg/article/2348200]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'meta-tags': {'og:site_name': 'Ptt 批踢踢實業坊', '...</td>\n",
       "      <td>[{'id': 0, 'reaction': '推', 'author': 'elfish1...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      version                                      canonical_url  \\\n",
       "0  1585778667  https://www.ptt.cc/bbs/HatePolitics/M.15829640...   \n",
       "1  1585781182  https://www.ptt.cc/bbs/HatePolitics/M.15829670...   \n",
       "2  1585781179  https://www.ptt.cc/bbs/HatePolitics/M.15829671...   \n",
       "3  1585781179  https://www.ptt.cc/bbs/HatePolitics/M.15829671...   \n",
       "4  1585781172  https://www.ptt.cc/bbs/HatePolitics/M.15829676...   \n",
       "\n",
       "                      title               author     connect_from  \\\n",
       "0  Re: [討論] 民進黨是如何騙倒台灣多數人的？        ilw4e (可以吃嗎？)  223.136.226.238   \n",
       "1        Re: [討論] 非法移工要怎麼處理  vancepeng (NIA新手玩家)   111.71.213.166   \n",
       "2  [討論] 我是柯粉,陳時中做的很好,但是可以提昇   CCTing (逃難回來囉,水有毒)  150.116.216.247   \n",
       "3       [討論] 其實台灣防疫工作作的不錯了！        fordmvp (無名氏)    59.126.170.22   \n",
       "4  [新聞] 新新聞》抗疫升級重要推手，侯友宜讓蘇貞        knnji (咖啡因過量)   110.50.151.162   \n",
       "\n",
       "         published_at       first_seen_at     last_updated_at  \\\n",
       "0 2020-03-01 00:14:44 2020-02-29 16:17:21 2020-04-02 06:04:27   \n",
       "1 2020-03-01 01:03:41 2020-02-29 17:19:58 2020-04-02 06:46:22   \n",
       "2 2020-03-01 01:05:14 2020-02-29 17:19:55 2020-04-02 06:46:19   \n",
       "3 2020-03-01 01:05:42 2020-02-29 17:19:54 2020-04-02 06:46:19   \n",
       "4 2020-03-01 01:14:03 2020-02-29 17:19:52 2020-04-02 06:46:12   \n",
       "\n",
       "                                     id                           producer_id  \\\n",
       "0  249c7ead-8254-11ea-8627-f23c92e71bad  5030a557-81fe-11ea-8627-f23c92e71bad   \n",
       "1  25e6b6e3-8254-11ea-8627-f23c92e71bad  5030a557-81fe-11ea-8627-f23c92e71bad   \n",
       "2  25e639e6-8254-11ea-8627-f23c92e71bad  5030a557-81fe-11ea-8627-f23c92e71bad   \n",
       "3  25e6255f-8254-11ea-8627-f23c92e71bad  5030a557-81fe-11ea-8627-f23c92e71bad   \n",
       "4  25e58aea-8254-11ea-8627-f23c92e71bad  5030a557-81fe-11ea-8627-f23c92e71bad   \n",
       "\n",
       "                                                text hashtags  \\\n",
       "0  ※ 引述《peoplerock (綽號暱稱)》之銘言：\\n:     最近開始看八卦板和政黑...       []   \n",
       "1  非法移工早就該處理了\\n現在可能成為防疫破口我也不意外\\n\\n為什麼會有非法勞工？\\n立法院...       []   \n",
       "2  先說結論\\n\\n陳時中我給他90分的高分\\n\\n但是柯文哲給的建議還是要聽\\n\\n公開透明才...       []   \n",
       "3  台灣對比日韓，很多地方都做的不錯了！\\n\\n口罩的管制、預防囤積、哄抬價格、禁止出口等\\n\\...       []   \n",
       "4  新新聞》抗疫升級重要推手，侯友宜讓蘇貞昌「聽話」20分鐘\\n\\n新新聞李順德  2020-0...       []   \n",
       "\n",
       "                                     urls keywords tags  \\\n",
       "0                                      []       []   []   \n",
       "1                                      []       []   []   \n",
       "2                                      []       []   []   \n",
       "3                                      []       []   []   \n",
       "4  [https://www.storm.mg/article/2348200]       []   []   \n",
       "\n",
       "                                            metadata  \\\n",
       "0  {'meta-tags': {'og:site_name': 'Ptt 批踢踢實業坊', '...   \n",
       "1  {'meta-tags': {'og:site_name': 'Ptt 批踢踢實業坊', '...   \n",
       "2  {'meta-tags': {'og:site_name': 'Ptt 批踢踢實業坊', '...   \n",
       "3  {'meta-tags': {'og:site_name': 'Ptt 批踢踢實業坊', '...   \n",
       "4  {'meta-tags': {'og:site_name': 'Ptt 批踢踢實業坊', '...   \n",
       "\n",
       "                                            comments image_urls  \n",
       "0  [{'id': 0, 'reaction': '推', 'author': 'zeuswel...        NaN  \n",
       "1  [{'id': 0, 'reaction': '推', 'author': 'yesfay'...        NaN  \n",
       "2  [{'id': 0, 'reaction': '推', 'author': 'Moratti...        NaN  \n",
       "3  [{'id': 0, 'reaction': '→', 'author': 'XXXXLIN...        NaN  \n",
       "4  [{'id': 0, 'reaction': '推', 'author': 'elfish1...        NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12322, 18)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('data/2020-05.json',encoding='utf-8')\n",
    "display(df.head())\n",
    "columns = df.columns\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106.104.74.133     86\n",
       "118.163.130.181    83\n",
       "24.18.90.133       75\n",
       "61.218.250.128     65\n",
       "123.194.157.160    65\n",
       "180.217.102.103    58\n",
       "125.14.165.240     55\n",
       "58.114.210.42      55\n",
       "101.8.215.217      52\n",
       "104.32.182.180     52\n",
       "Name: connect_from, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.connect_from.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wupaul (台灣好孩子之蜜蜂俠粉絲)       141\n",
       "kero2377 (愛gina福利熊)        136\n",
       "yuxds (與人為善)               132\n",
       "kuninaka (家入レオ推廣大使)        124\n",
       "adagiox (adagio)           117\n",
       "Fant1408 (     )           113\n",
       "pujipuji ()                107\n",
       "sunyeah (   湯元嗎)           107\n",
       "Rrrxddd (有噴 鋼鐵LULU粉 RR)    105\n",
       "TheoEpstein (Cubs)         103\n",
       "Name: author, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.author.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get stop words\n",
    "with open('utils/stopword.txt',encoding='utf-8') as f:\n",
    "    lines = f.read().splitlines()\n",
    "\n",
    "unwanted_ele = [' ',':','\\n']\n",
    "unwanted_ele.extend(lines)\n",
    "jieba.set_dictionary('utils/dict.txt.big.txt')    \n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def dePuntufy(text):\n",
    "    punc = \"※︰,()！？｡。＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃《》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.\"\n",
    "    return re.sub(r\"[%s]+\" %punc, \" \", text)\n",
    "\n",
    "def deUrl(text):\n",
    "    text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    text = deUrl(text)\n",
    "    text = deEmojify(text)\n",
    "    text = dePuntufy(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    string = '|'.join(jieba.cut(text, cut_all=False, HMM=True))\n",
    "    li = string.split('|')\n",
    "    #remove unwanted elements e.g. ' '\n",
    "    tmp = [ele for ele in li if ele not in unwanted_ele]\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from D:\\AIS3\\iorg\\AIS3\\utils\\dict.txt.big.txt ...\n",
      "Dumping model to file cache C:\\Users\\Matto\\AppData\\Local\\Temp\\jieba.u494706cce827de0841cc9373532f60de.cache\n",
      "Loading model cost 1.569 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[most frequent vocabularies]\n",
      "人: 6423\n",
      "台灣: 5236\n",
      "說: 4875\n",
      "中國: 3793\n",
      "黨: 3630\n",
      "柯文哲: 3272\n",
      "新聞: 3260\n",
      "好: 3047\n",
      "引述: 2977\n",
      "中: 2935\n",
      "防疫: 2894\n",
      "疫情: 2787\n",
      "一個: 2537\n",
      "陳: 2491\n",
      "民眾: 2458\n",
      "真的: 2354\n",
      "美國: 2295\n",
      "時: 2173\n",
      "柯粉: 2103\n",
      "國民黨: 2094\n",
      "政府: 2069\n",
      "肺炎: 2022\n",
      "民進黨: 1996\n",
      "知道: 1946\n",
      "武漢: 1938\n",
      "請: 1901\n",
      "─: 1861\n",
      "柯: 1743\n",
      "市長: 1679\n",
      "做: 1671\n",
      "日: 1648\n",
      "政治: 1560\n",
      "台北: 1554\n",
      "內容: 1540\n",
      "轉錄: 1502\n",
      "月: 1489\n",
      "後: 1473\n",
      "口罩: 1468\n",
      "問題: 1434\n",
      "年: 1410\n",
      "今天: 1409\n",
      "想: 1389\n",
      "記者: 1262\n",
      "表示: 1240\n",
      "韓國瑜: 1219\n",
      "是不是: 1198\n",
      "2020: 1186\n",
      "文章: 1168\n",
      "總統: 1162\n",
      "看到: 1147\n"
     ]
    }
   ],
   "source": [
    "# ngram_range=(min,max), default: 1-gram => (1,1)\n",
    "count = CountVectorizer(preprocessor=preprocess,tokenizer=tokenize)\n",
    "\n",
    "doc = df['text'].values\n",
    "doc_bag = count.fit_transform(doc).toarray()\n",
    "\n",
    "print(\"[most frequent vocabularies]\")\n",
    "bag_cnts = np.sum(doc_bag, axis=0)\n",
    "top = 50\n",
    "# [::-1] reverses a list since sort is in ascending order\n",
    "for tok, v in zip(count.inverse_transform(np.ones(bag_cnts.shape[0]))[0][bag_cnts.argsort()[::-1][:top]], np.sort(bag_cnts)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(preprocessor=preprocess,tokenizer=tokenize)\n",
    "count.fit(doc)\n",
    "# dictionary is stored in vocabulary_\n",
    "BoW = count.vocabulary_\n",
    "#print('[vocabulary]\\n{}'.format(BoW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocabularies with smallest idf scores]\n",
      "人: 2.12\n",
      "說: 2.28\n",
      "台灣: 2.45\n",
      "引述: 2.53\n",
      "好: 2.67\n",
      "黨: 2.83\n",
      "一個: 2.85\n",
      "中: 2.85\n",
      "新聞: 2.86\n",
      "真的: 2.86\n",
      "疫情: 2.88\n",
      "柯文哲: 2.95\n",
      "防疫: 2.95\n",
      "中國: 3.00\n",
      "民眾: 3.07\n",
      "知道: 3.08\n",
      "內容: 3.11\n",
      "時: 3.15\n",
      "政府: 3.16\n",
      "肺炎: 3.18\n",
      "陳: 3.20\n",
      "武漢: 3.22\n",
      "做: 3.25\n",
      "後: 3.28\n",
      "民進黨: 3.29\n",
      "想: 3.32\n",
      "今天: 3.34\n",
      "柯粉: 3.35\n",
      "國民黨: 3.37\n",
      "台北: 3.38\n",
      "市長: 3.39\n",
      "柯: 3.40\n",
      "日: 3.41\n",
      "問題: 3.41\n",
      "政治: 3.41\n",
      "美國: 3.44\n",
      "2020: 3.46\n",
      "月: 3.46\n",
      "是不是: 3.48\n",
      "看到: 3.49\n",
      "表示: 3.50\n",
      "再: 3.51\n",
      "年: 3.51\n",
      "記者: 3.52\n",
      "請: 3.55\n",
      "新聞來源: 3.57\n",
      "總統: 3.65\n",
      "報導: 3.65\n",
      "根本: 3.66\n",
      "前: 3.68\n",
      "\n",
      "[vocabularies with highest tf-idf scores]\n",
      "人: 252.3613660431157\n",
      "台灣: 233.46700857153797\n",
      "說: 207.63586637670826\n",
      "中國: 205.2545741361716\n",
      "黨: 193.16229987139985\n",
      "柯文哲: 181.38966511434032\n",
      "好: 157.03620555609425\n",
      "新聞: 154.91093833751907\n",
      "防疫: 154.63572887037995\n",
      "中: 147.62486132947174\n",
      "陳: 140.74798398194105\n",
      "疫情: 140.219688788371\n",
      "民眾: 139.99425549751814\n",
      "柯粉: 137.80482157454892\n",
      "美國: 136.10678700351443\n",
      "一個: 135.79172471482127\n",
      "真的: 132.91675636199255\n",
      "引述: 132.02532150099415\n",
      "國民黨: 123.61400938107462\n",
      "時: 121.79662022487733\n",
      "民進黨: 119.21864238473387\n",
      "知道: 116.0243188055953\n",
      "政府: 115.34980001908369\n",
      "請: 112.13648951543485\n",
      "肺炎: 111.77336043709387\n",
      "柯: 111.59019593867016\n",
      "武漢: 110.79033234238949\n",
      "市長: 102.19085026504408\n",
      "轉錄: 100.54990562490828\n",
      "做: 99.6862965094174\n",
      "口罩: 99.4000499128434\n",
      "政治: 96.50161852897179\n",
      "想: 89.0519167120491\n",
      "是不是: 88.6806391013728\n",
      "問題: 88.55074721401759\n",
      "台北: 88.40826893465604\n",
      "韓國瑜: 85.9528387031806\n",
      "今天: 85.80531537326858\n",
      "日: 85.13937313429572\n",
      "月: 82.7132585818382\n",
      "後: 80.53860121551514\n",
      "年: 79.83123941806464\n",
      "出國: 79.26212981320076\n",
      "看到: 79.08022878501461\n",
      "總統: 78.39319069662524\n",
      "文章: 78.38783824187766\n",
      "幹: 77.89767703450497\n",
      "內容: 77.66183195689304\n",
      "病毒: 74.92491338219264\n",
      "國家: 73.80574641700649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(preprocessor = preprocess,tokenizer=tokenize)\n",
    "\n",
    "tfidf.fit(doc)\n",
    "\n",
    "top = 50\n",
    "# get idf score of vocabularies\n",
    "idf = tfidf.idf_\n",
    "print('[vocabularies with smallest idf scores]')\n",
    "sorted_idx = idf.argsort()\n",
    "\n",
    "for i in range(top):\n",
    "    print('%s: %.2f' %(tfidf.get_feature_names()[sorted_idx[i]], idf[sorted_idx[i]]))\n",
    "\n",
    "doc_tfidf = tfidf.transform(doc).toarray()\n",
    "tfidf_sum = np.sum(doc_tfidf, axis=0)\n",
    "print(\"\\n[vocabularies with highest tf-idf scores]\")\n",
    "for tok, v in zip(tfidf.inverse_transform(np.ones(tfidf_sum.shape[0]))[0][tfidf_sum.argsort()[::-1]][:top], \\\n",
    "                        np.sort(tfidf_sum)[::-1][:top]):\n",
    "    print('{}: {}'.format(tok, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "jieba: file does not exist: D:\\AIS3\\iorg\\AIS3\\stopword.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-4a4da72ccc2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mjieba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_stop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopword.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\jieba\\analyse\\__init__.py\u001b[0m in \u001b[0;36mset_stop_words\u001b[1;34m(stop_words_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mset_stop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mdefault_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_stop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mdefault_textrank\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_stop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\jieba\\analyse\\tfidf.py\u001b[0m in \u001b[0;36mset_stop_words\u001b[1;34m(self, stop_words_path)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mabs_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_abs_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"jieba: file does not exist: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mabs_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: jieba: file does not exist: D:\\AIS3\\iorg\\AIS3\\stopword.txt"
     ]
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "jieba.analyse.set_stop_words('utils/stopword.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('default:{}'.format(jieba.analyse.extract_tags(doc[0],topK=10)))\n",
    "print('tf-idf:{}'.format(jieba.analyse.textrank(doc[0],topK=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordlist = list()\n",
    "for i in range(df.shape[0]):\n",
    "        li = jieba.analyse.extract_tags(doc[i],topK=10)\n",
    "        #li = jieba.analyse.textrank(doc[i],topK=10)\n",
    "        keywordlist.extend(li)\n",
    "        \n",
    "cts = Counter(keywordlist)\n",
    "df_freq = pd.DataFrame.from_dict(cts, orient='index').reset_index()\n",
    "display(df_freq.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_path = 'utils/MSJH.TTC'\n",
    "\n",
    "wc = WordCloud(background_color='white',font_path=font_path,max_words=30,stopwords=STOPWORDS.update(lines))\n",
    "\n",
    "seg_list=' '.join(keywordlist)\n",
    "wc.generate(seg_list)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop zero total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_drop_0_total = df[df.total !=0 ]\n",
    "print(df_drop_0_total.shape)\n",
    "df_drop_0_total = df_drop_0_total[df_drop_0_total.likesAtPosting !='N/A' ]\n",
    "print(df_drop_0_total.shape)\n",
    "df_drop_less_1000 = df_drop_0_total[df_drop_0_total.likesAtPosting.astype(int) > 1000 ]\n",
    "print(df_drop_less_1000.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blog_cts = df.blogName.value_counts()\n",
    "top30 = blog_cts[:30].keys()\n",
    "blog_name = df_f.blogName.unique()\n",
    "print(len(blog_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blog = df_drop_0_total.groupby('blogName')\n",
    "for i in range(30):\n",
    "    print(top30[i])\n",
    "    print(blog.get_group(top30[i])['likes'].sum()/blog.get_group(top30[0]).shape[0])*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove icon emoji url\n",
    "df['linkText'] = df['linkText'].apply(lambda x: preprocess(x))\n",
    "df['message'] = df['message'].apply(lambda x: preprocess(x))\n",
    "df['description'] = df['description'].apply(lambda x: preprocess(x))\n",
    "\n",
    "## cut word\n",
    "df['linkText'] = df['linkText'].apply(lambda x: '|'.join(jieba.cut(x, cut_all=False, HMM=True)))\n",
    "df['message'] = df['message'].apply(lambda x: '|'.join(jieba.cut(x, cut_all=False, HMM=True)))\n",
    "df['description'] = df['description'].apply(lambda x: ''.join(jieba.cut(x, cut_all=False, HMM=True)))\n",
    "key = ['total', 'likes', 'comments', 'shares', 'love', 'wow', 'haha', 'sad', 'angry', 'thankful']\n",
    "for i in key:\n",
    "    df[i] = df['interaction'].apply(lambda x: x[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}